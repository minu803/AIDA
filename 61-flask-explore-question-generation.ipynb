{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flask and question generation\n",
    "\n",
    "This version of the flask UI builds on notebook 60 to add question generation. Questions are generated with the updated prompt, which generates questions along a set of dichotomous dimensions.\n",
    "\n",
    "Issues remaining:\n",
    "- adding in transcript matching\n",
    "- using pause detection in speech for more robust question timing\n",
    "- faster question generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from getpass import getpass\n",
    "openai_api_key = getpass()\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with pyaudio package\n",
    "\n",
    "Dependencies that are necessary in addition to those that come with anaconda below. PyAudio has some issues installing and likely needs portaudio first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install SpeechRecognition\n",
    "# !pip install openai\n",
    "# !pip install whisper\n",
    "# !pip install langchain\n",
    "\n",
    "# !conda install pyaudio\n",
    "# -or-\n",
    "# !apt-get install portaudio19-dev\n",
    "# !pip install --upgrade pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr # https://pypi.org/project/SpeechRecognition/\n",
    "import queue\n",
    "import time\n",
    "import threading\n",
    "import sys\n",
    "import openai\n",
    "import pyaudio\n",
    "import whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UI design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updates design - better looking UI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./templates/index.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./templates/index.html\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Transcriber</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            background-color: #f4f4f4;\n",
    "            color: #333;\n",
    "        }\n",
    "\n",
    "        h1 {\n",
    "            text-align: center;\n",
    "            padding: 20px;\n",
    "            color: #4a4a4a;\n",
    "        }\n",
    "\n",
    "        p {\n",
    "            padding: 20px;\n",
    "            font-size: 16px;\n",
    "        }\n",
    "\n",
    "        # toggle-button {\n",
    "            display: block;\n",
    "            width: 200px;\n",
    "            height: 50px;\n",
    "            margin: 20px auto;\n",
    "            background-color: #3498db;\n",
    "            color: #fff;\n",
    "            border: none;\n",
    "            border-radius: 5px;\n",
    "            font-size: 18px;\n",
    "            transition: background 0.2s ease;\n",
    "        }\n",
    "\n",
    "        # toggle-button:hover {\n",
    "            background-color: #2980b9;\n",
    "            cursor: pointer;\n",
    "        }\n",
    "\n",
    "        .recording {\n",
    "            background-color: #e74c3c;\n",
    "        }\n",
    "\n",
    "        .recording:hover {\n",
    "            background-color: #c0392b;\n",
    "        }\n",
    "    </style>\n",
    "    <script type=\"text/javascript\">\n",
    "        var source = new EventSource(\"/updates\");\n",
    "        source.onmessage = function(event) {\n",
    "            document.getElementById(\"updated-text\").innerHTML = event.data;\n",
    "        };\n",
    "        var gptSource = new EventSource(\"/gpt_updates\");\n",
    "        gptSource.onmessage = function(event) {\n",
    "            document.getElementById(\"gpt-text\").innerHTML = event.data;\n",
    "        };\n",
    "        function toggleVariable() {\n",
    "            var xhr = new XMLHttpRequest();\n",
    "            xhr.open('POST', '/toggle', true);\n",
    "            xhr.onreadystatechange = function() {\n",
    "                if (xhr.readyState === XMLHttpRequest.DONE && xhr.status === 200) {\n",
    "                    console.log('Toggle success');\n",
    "                    // Change button color and text\n",
    "                    var button = document.getElementById(\"toggle-button\");\n",
    "                    button.classList.toggle(\"recording\");\n",
    "                    if (button.innerHTML === \"Start Recording\") {\n",
    "                        button.innerHTML = \"Stop Recording\";\n",
    "                    } else {\n",
    "                        button.innerHTML = \"Start Recording\";\n",
    "                    }\n",
    "                }\n",
    "            };\n",
    "            xhr.send()\n",
    "        }\n",
    "    </script>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Transcriber</h1>\n",
    "    <button id=\"toggle-button\" onclick=\"toggleVariable()\">Start Recording</button>\n",
    "    <h1>Transcription</h1>\n",
    "    <p id=\"updated-text\"></p>\n",
    "    <h1>Generated Questions</h1>\n",
    "    <p id=\"gpt-text\"></p>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "####################\n",
    "global gpt_output\n",
    "gpt_output = \"\"\n",
    "# gpt_output += \"<hr>\" + response.content\n",
    "global sentence_counter\n",
    "sentence_counter = 0\n",
    "\n",
    "import speech_recognition as sr # https://pypi.org/project/SpeechRecognition/\n",
    "import queue\n",
    "from flask import Flask, render_template, Response\n",
    "import time\n",
    "import threading\n",
    "import sys\n",
    "import openai\n",
    "openai.api_key = 'add you key here'\n",
    "\n",
    "####################\n",
    "prompt_string = \"\"\"\n",
    "Act as an expert in dialogic reading. I will give you a snippet from a children’s story, and you will give me 5 dialogic \n",
    "reading questions for a 4 year-old child.\n",
    "\n",
    "When generating these questions, I want you to consider a few things. \n",
    "\n",
    "Firstly, questions can be either “concrete” or “abstract”. Concrete questions ask about perceptually obvious/physical \n",
    "aspects and abstract questions ask about non-obvious/conceptual/cognitive/emotional aspects \n",
    "(with the latter typically requiring deeper reasoning/causal inference). \n",
    "You can also think of concrete questions as focusing on explicit information, where abstract questions focus \n",
    "on implicit information.\n",
    "\n",
    "Secondly, questions can be child-focused or book-focused. Child-focused questions focus on the child's experiences,\n",
    "feelings, and connections to the material. The purpose of these questions is to help the child connect the content \n",
    "of the book with their own life, emotions, or previous knowledge. These questions are particularly useful for helping \n",
    "children build empathy and understanding, as well as developing personal connections with literature. \n",
    "Book-focused questions deal directly with the content, themes, or structures within the book itself.\n",
    "These questions may be about the plot, characters, settings, or author's intent, among other things. \n",
    "These questions help children deepen their understanding of the book and develop critical reading skills. \n",
    "They can foster comprehension, recall, analysis, and interpretation.\n",
    "\n",
    "Finally, questions may be open-ended or close-ended. Close-ended questions will typically require one word or phrase\n",
    "answers, and will have a correct answer. Open-ended questions can require as long of an answer as needed and might not\n",
    "have a “correct” answer.\n",
    "\n",
    "Bearing this in mind, please generate questions that fall into these categories. Questions can and should be any\n",
    "combination of the above dimensions. Please generate 5 questions which range across all dimensions and include variety\n",
    "in the above dimensions. Do not give me anything other than the list the questions, numbered 1-5. Do not give potential answers.\n",
    "Do not state which of the above dimensions correspond to each question.\n",
    "\n",
    "Please also keep in mind that the child that the questions are for is 4 years old, and use an appropriate vocabulary \n",
    "and level of difficulty for a child this age.\n",
    "\n",
    "The story snippet to generate questions for is: \n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "Set up queues and microphone for recording\n",
    "'''\n",
    "\n",
    "# Queues\n",
    "# Audio queue stores audio data from mic/recorder\n",
    "# Result queue stores transcribed output\n",
    "audio_queue  = queue.Queue()\n",
    "result_queue = queue.Queue()\n",
    "transcript_queue = queue.Queue()\n",
    "\n",
    "# variables to control app\n",
    "global text_all\n",
    "text_all = \"\" # output result string\n",
    "break_threads = False # quit out\n",
    "\n",
    "# SETUP MICROPHONE RECORDING\n",
    "# Reference: https://github.com/Uberi/speech_recognition/blob/master/examples/background_listening.py\n",
    "# General Reference: https://github.com/Uberi/speech_recognition/blob/master/reference/library-reference.rst\n",
    "\n",
    "# Init mic and recorder\n",
    "# Recorder will use the mic and callback function\n",
    "# to constantly listen for audio and add it to the audio_queue\n",
    "print('Detected Microphones:')\n",
    "print(sr.Microphone.list_microphone_names()) # list microphones\n",
    "mic = sr.Microphone()\n",
    "recorder = sr.Recognizer()\n",
    "sample_rate = mic.SAMPLE_RATE # use default sample rate for mic, whisper API can handle it\n",
    "\n",
    "# Mic/Recorder Settings\n",
    "pause_threshold = .5\n",
    "energy = 300\n",
    "dynamic_energy = False\n",
    "\n",
    "# Represents the energy level threshold for sounds. Values below this threshold are considered silence, and values above this threshold are considered speech\n",
    "recorder.energy_threshold = energy\n",
    "\n",
    "# Represents the minimum length of silence (in seconds) that will register as the end of a phrase\n",
    "# Smaller values result in the recognition completing more quickly, but might result in slower speakers being cut off\n",
    "recorder.pause_threshold  = pause_threshold\n",
    "\n",
    "# Automatically increase/decrease energy to account for ambient noise\n",
    "recorder.dynamic_energy_threshold = dynamic_energy\n",
    "\n",
    "# Adjusts the energy threshold dynamically\n",
    "with mic as source:\n",
    "    recorder.adjust_for_ambient_noise(source)\n",
    "\n",
    "# 1ST THREAD - This is called from the background thread\n",
    "# takes data from mic and adds directly to audio queue\n",
    "def record_callback(_, audio:sr.AudioData):\n",
    "    global toggle_variable\n",
    "    if toggle_variable:\n",
    "        # only add data to audio queue if button has been pressed\n",
    "        data = audio.get_raw_data()\n",
    "        audio_queue.put_nowait(data)\n",
    "\n",
    "# Start listening in another thread\n",
    "# Spawns a thread to repeatedly record phrases from mic\n",
    "# phrase time limit - maximum length of recorded phrases (seconds)\n",
    "recorder.listen_in_background(mic, record_callback, phrase_time_limit=10)\n",
    "print(\"Microphone ready!\")\n",
    "\n",
    "'''\n",
    "Define some necessary functions\n",
    "'''\n",
    "\n",
    "# Transcribe audio with WHISPER\n",
    "def transcribe_audio(file_path):\n",
    "    with open(file_path, \"rb\") as audio_file:\n",
    "        transcript = openai.Audio.transcribe(\"whisper-1\", audio_file, language='en', \n",
    "                                             prompt=\"Children's story\" + text_all[-200:])\n",
    "    return transcript[\"text\"]\n",
    "\n",
    "# Get audio data stored in audio_queue\n",
    "# Pulls data from the queue that was put there with record_callback()\n",
    "# Will pull data until queue is empty or (elapsed-time > min_time)\n",
    "def get_all_audio(min_time=-1):\n",
    "    audio = bytes()\n",
    "    got_audio = False\n",
    "    time_start = time.time()\n",
    "    while not got_audio or time.time() - time_start < min_time: # min time unused right now\n",
    "        # loops as long as there's something in the audio queue\n",
    "        while not audio_queue.empty():\n",
    "            audio += audio_queue.get() # pull data from audio queue\n",
    "            got_audio = True\n",
    "\n",
    "    data = sr.AudioData(audio,sample_rate,2)\n",
    "    return data\n",
    "\n",
    "# Get data from audio queue, save it as .wav, and transcribe .wav\n",
    "# Adds transcribed .wav to result queue\n",
    "def transcribe_data_from_queue():\n",
    "    audio_data = get_all_audio() # get audio data from queue\n",
    "\n",
    "    # Save audio data as .wav\n",
    "    with open(\"latest.wav\", \"wb\") as f:\n",
    "        f.write(audio_data.get_wav_data())\n",
    "\n",
    "    # Transcribe the saved audio file\n",
    "    transcript_text = transcribe_audio('./latest.wav')\n",
    "\n",
    "    # Add transcription to queue\n",
    "    result_queue.put_nowait(transcript_text)\n",
    "\n",
    "\n",
    "# Loop to run transcribe() in its own thread\n",
    "# Continuosly grabs audio, transcribes it, and adds output to result queue\n",
    "# status and break_threads need to be global?\n",
    "def transcribe_loop():\n",
    "    while True:\n",
    "        if break_threads:\n",
    "            break\n",
    "        else:\n",
    "            transcribe_data_from_queue()\n",
    "    sys.exit()\n",
    "\n",
    "# Loop to pull results and print it\n",
    "# Continuosly grabs transcribed from result_queue and prints it\n",
    "# result_queue, text_all, break_threads need to be global?\n",
    "def print_result_loop():\n",
    "    while True:\n",
    "        result = result_queue.get() # get data from result queue\n",
    "        transcript_queue.put_nowait(result)\n",
    "        print(result)\n",
    "        global text_all\n",
    "        text_all += result + '<br>'    # append it to output result string\n",
    "\n",
    "        # If output result string too long, reset it\n",
    "        #if len(text_all) > 2000:\n",
    "        #    text_all = \"\"\n",
    "\n",
    "        # Quit if 'stop' is said\n",
    "        # need better way to quit threads?\n",
    "        if result.lower().find('stop') > -1:\n",
    "            #text_all += '. breaking...'\n",
    "            break_threads = True\n",
    "            break\n",
    "    sys.exit()\n",
    "    \n",
    "####################\n",
    "# Generate GPT-4 output and update the global variable\n",
    "def chat_with_chatgpt(transcript, model=\"gpt-3.5-turbo\"):\n",
    "    response = openai.ChatCompletion.create(\n",
    "      model=model,\n",
    "      messages=[{\"role\": \"user\", \"content\": prompt_string+transcript}]  \n",
    "    )\n",
    "    message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return message\n",
    "    \n",
    "def generate_loop():\n",
    "    global gpt_output\n",
    "    results = []\n",
    "    while True:\n",
    "        result = transcript_queue.get()\n",
    "        results.append(result)\n",
    "\n",
    "        # Generate GPT-4 output every five sentences\n",
    "        if len(results) >= 3:\n",
    "            generated_qs = chat_with_chatgpt(''.join(results[-1]))\n",
    "            results = []   \n",
    "            \n",
    "            gpt_output = generated_qs.replace(\"\\n\", \"<br>\") + \"<br>\"\n",
    "            \n",
    "    sys.exit()\n",
    "'''\n",
    "Run the flask app\n",
    "'''\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.debug = True\n",
    "\n",
    "threading.Thread(target=print_result_loop).start()# print output thread\n",
    "#thread1.daemon = True  # Set the thread to daemon so it ends when the main thread ends\n",
    "#thread1.start()\n",
    "threading.Thread(target=transcribe_loop).start()   # transcribe thread\n",
    "#thread2.daemon = True  # Set the thread to daemon so it ends when the main thread ends\n",
    "#thread2.start()\n",
    "threading.Thread(target=generate_loop).start()   # transcribe thread\n",
    "\n",
    "# Global variable\n",
    "global toggle_variable\n",
    "toggle_variable = False\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/toggle', methods=['POST'])\n",
    "def toggle():\n",
    "    global toggle_variable\n",
    "    toggle_variable = not toggle_variable\n",
    "    return 'Success'\n",
    "\n",
    "@app.route('/updates')\n",
    "def updates():\n",
    "    def generate_updates():\n",
    "        while True:\n",
    "            # Generate the updated text here\n",
    "            global text_all\n",
    "            global toggle_variable\n",
    "            updated_text = text_all\n",
    "\n",
    "            # Yield the SSE-formatted response\n",
    "            yield f\"data: {updated_text}\\n\\n\"\n",
    "            \n",
    "            # Wait before sending the next update\n",
    "            #time.sleep(0.2)\n",
    "\n",
    "    return Response(generate_updates(), mimetype='text/event-stream')\n",
    "\n",
    "####################\n",
    "@app.route('/gpt_updates')\n",
    "def gpt_updates():\n",
    "    def generate_gpt_updates():\n",
    "        while True:\n",
    "            # Generate the updated GPT-4 output here\n",
    "            global gpt_output\n",
    "            updated_gpt_output = gpt_output\n",
    "\n",
    "            # Yield the SSE-formatted response\n",
    "            yield f\"data: {updated_gpt_output}\\n\\n\"\n",
    "            \n",
    "            # Wait before sending the next update\n",
    "            #time.sleep(0.2)\n",
    "\n",
    "    return Response(generate_gpt_updates(), mimetype='text/event-stream')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=5050, use_reloader=False)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run(use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Microphones:\n",
      "['MacBook Pro Microphone', 'MacBook Pro Speakers', 'ZoomAudioDevice']\n",
      "Microphone ready!\n",
      " * Serving Flask app 'app'\n",
      " * Debug mode: on\n",
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5050\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python app.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
